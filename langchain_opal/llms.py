"""OPAL large language models."""

import logging
from typing import (
    Any,
    AsyncIterator,
    Callable,
    Dict,
    Iterator,
    List,
    Literal,
    Mapping,
    Optional,
    Union,
)

import os
import httpx
from httpx import Timeout

from langchain_core.callbacks import (
    AsyncCallbackManagerForLLMRun,
    CallbackManagerForLLMRun,
)
from langchain_core.language_models.chat_models import (
    LangSmithParams,
    # generate_from_stream,
)

from langchain_core.utils import from_env, secret_from_env
from pydantic import Field, PrivateAttr, SecretStr, model_validator
from typing_extensions import Self

from langchain_core.callbacks.manager import CallbackManagerForLLMRun
from langchain_core.language_models.llms import LLM

logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)
#handler = logging.FileHandler('opal_llm.log')
#handler.setLevel(logging.DEBUG)
#formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
#handler.setFormatter(formatter)
#logger.addHandler(handler)


DEFAULT_REQUEST_TIMEOUT = 60.0

class OpalLLM(LLM):
    """A custom chat model that echoes the first `n` characters of the input.

    When contributing an implementation to LangChain, carefully document
    the model including the initialization parameters, include
    an example of how to initialize the model and include any relevant
    links to the underlying models documentation or API.

    Example:

        .. code-block:: python

            model = CustomChatModel(n=2)
            result = model.invoke([HumanMessage(content="hello")])
            result = model.batch([[HumanMessage(content="hello")],
                            [HumanMessage(content="world")]])
    """
    model_name: str = "gpt-4o"
    """Model name to use."""

    temperature: Optional[float] = 0.2
    """The temperature of the model. Increasing the temperature will
    make the model answer more creatively. (Default: 0.2)"""

    stop: Optional[List[str]] = None
    """Sets the stop tokens to use."""

    top_p: Optional[float] = 0.5
    """Works together with top-k. A higher value (e.g., 0.95) will lead
    to more diverse text, while a lower value (e.g., 0.5) will
    generate more focused and conservative text. (Default: 0.5)"""

    api_base: Optional[str] = "https://linkeddata.uriburner.com"
    """The base URL for OPAL API."""

    finetune: Optional[str] = "system-data-twingler-config"
    """Finetune mode"""

    funcs_list: Optional[List[str]] = ["UB.DBA.sparqlQuery", "DB.DBA.vos_howto_search", "Demo.demo.execute_sql_query", "DB.DBA.graphqlQuery"]
    """Functions list"""

    request_timeout: float = DEFAULT_REQUEST_TIMEOUT
    """The timeout for making http request to llamafile API server"""

    openlink_api_key: SecretStr = Field(
        alias="openlink_api_key",
        default_factory=secret_from_env(
            "OPENLINK_API_KEY",
            error_message=(
                "You must specify an openlink api key. "
                "You can pass it an argument as `openlink_api_key=...` or "
                "set the environment variable `OPENLINK_API_KEY`."
            ),
        ),
    )
    """OpenLink API key.

    Automatically read from env variable `OPENLINK_API_KEY` if not provided.
    """

    openai_api_key: SecretStr = Field(
        alias="openai_api_key",
        default_factory=secret_from_env(
            "OPENAI_API_KEY",
            error_message=(
                "You must specify an openai api key. "
                "You can pass it an argument as `openai_api_key=...` or "
                "set the environment variable `OPENAI_API_KEY`."
            ),
        ),
    )
    """OpenAI API key.

    Automatically read from env variable `OPENAI_API_KEY` if not provided.
    """

    _chat_id: str = None
    continue_chat: bool = False


    @property
    def _default_params(self) -> Dict[str, Any]:
        """Get the default parameters for calling Opal."""
        return {
            "model": self.model_name,
            "type": "user",
            "temperature": self.temperature,
            "top_p": self.top_p,
            "call": self.funcs_list,
            "fine_tune": self.finetune,
        }

    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        """Run the LLM on the given input.

        Args:
            prompt: The prompt to pass into the model.

        Returns:
            The string generated by the model..
        """
        if stop is not None:
            raise ValueError("stop kwargs are not permitted.")

        payload = self._default_params
        payload["apiKey"] = self.openai_api_key.get_secret_value()

        headers = {
            "accept": "application/json",
            "content-type": "application/json",
            "authorization": f"Bearer {self.openlink_api_key.get_secret_value()}",
        }
        api_url = f"{self.api_base}/chat/api/chatCompletion"
        chat_id = self._chat_id if self.continue_chat else None

        if chat_id is None:
            payload["chat_id"] = self.finetune
            with httpx.Client(timeout=Timeout(self.request_timeout)) as client:
                response = client.post(
                    url=api_url,
                    json=payload,
                    headers=headers,
                )
                response.raise_for_status()
                raw = response.json()
                chat_id = raw.get("chat_id")
                if chat_id is None:
                    raise (ValueError("Could not create Chat"))
                if self.continue_chat:
                    self._chat_id = chat_id

        payload["chat_id"] = chat_id
        payload["question"] = prompt

        logger.info(f"Sending request to {api_url} with payload: {payload} ")
        with httpx.Client(timeout=Timeout(self.request_timeout)) as client:
            response = client.post(
                url=api_url,
                json=payload,
                headers=headers,
            )
            response.raise_for_status()
            raw = response.json()
            kind = raw.get("kind")
            message = raw.get("data", "")
            if kind is not None and kind == "error":
                raise (ValueError(message))
            # if run_manager:
            #     run_manager.on_llm_end(response=LLMResult()) 
            logger.info(f"Received response: {message}")
            return message

    async def _acall(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        """Run the LLM on the given input.

        Args:
            prompt: The prompt to pass into the model.

        Returns:
            The string generated by the model..
        """
        if stop is not None:
            raise ValueError("stop kwargs are not permitted.")

        payload = self._default_params
        payload["apiKey"] = self.openai_api_key.get_secret_value()

        headers = {
            "accept": "application/json",
            "content-type": "application/json",
            "authorization": f"Bearer {self.openlink_api_key.get_secret_value()}",
        }
        api_url = f"{self.api_base}/chat/api/chatCompletion"
        chat_id = self._chat_id if self.continue_chat else None

        if chat_id is None:
            payload["chat_id"] = self.finetune
            async with httpx.AsyncClient(timeout=Timeout(self.request_timeout)) as client:
                response = await client.post(
                    url=api_url,
                    json=payload,
                    headers=headers,
                )
                response.raise_for_status()
                raw = response.json()
                chat_id = raw.get("chat_id")
                if chat_id is None:
                    raise (ValueError("Could not create Chat"))
                if self.continue_chat:
                    self._chat_id = chat_id

        payload["chat_id"] = chat_id
        payload["question"] = prompt

        logger.info(f"Sending request to {api_url} with payload: {payload} ")
        async with httpx.AsyncClient(timeout=Timeout(self.request_timeout)) as client:
            response = await client.post(
                url=api_url,
                json=payload,
                headers=headers,
            )
            response.raise_for_status()
            raw = response.json()
            kind = raw.get("kind")
            message = raw.get("data", "")
            if kind is not None and kind == "error":
                raise (ValueError(message))
            # if run_manager:
            #     run_manager.on_text(message)
            logger.info(f"Received response: {message}")
            return message



    @property
    def _identifying_params(self) -> Dict[str, Any]:
        """Return a dictionary of identifying parameters."""
        return {
            # The model name allows users to specify custom token counting
            # rules in LLM monitoring applications (e.g., in LangSmith users
            # can provide per token pricing for their model and monitor
            # costs for the given LLM.)
            "model_name": self.model_name,
            "api_base": self.api_base,
            "finetune": self.finetune,
            "funcs_list": self.funcs_list,
            "continue_chat": self.continue_chat,
        }


    @property
    def _llm_type(self) -> str:
        """Get the type of language model used by this chat model. Used for logging purposes only."""
        return "OpalLLM"


    @property
    def lc_secrets(self) -> Dict[str, str]:
        """A map of constructor argument names for secret IDs.
        """
        return {
            "openlink_api_key": "OPENLINK_API_KEY",
            "openai_api_key": "OPENAI_API_KEY",
        }

    def _get_ls_params(
        self, stop: Optional[List[str]] = None, **kwargs: Any
    ) -> LangSmithParams:
        """Get standard params for tracing."""
        params = super()._get_ls_params(stop=stop, **kwargs)
        ls_params = LangSmithParams(
            ls_provider="opal",
            ls_model_name=self.model_name,
            ls_temperature=params.get("temperature", self.temperature),
        )
        if ls_stop := stop or params.get("stop", None) or self.stop:
            ls_params["ls_stop"] = ls_stop
        return ls_params


